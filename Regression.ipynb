{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT"
      ],
      "metadata": {
        "id": "QInRSDDEmY1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cYxSwvcMmekH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a statistical method used to model the relationship between two variables: one independent (predictor) variable and one dependent (response) variable, assuming a linear relationship.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - The key assumptions are:\n",
        "\n",
        "    - Linearity: The relationship between X and Y is linear.\n",
        "    - Independence: Observations are independent.\n",
        "    - Homoscedasticity: The variance of residuals is constant across all levels of X.\n",
        "    - Normality: Residuals are normally distributed.\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "  - The coefficient m represents the slope of the regression line, indicating the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "  - The intercept c represents the value of Y when X is 0. It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - The slope m is calculated using the formula:\n",
        "  - m = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}}\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "  - The least squares method minimizes the sum of squared differences between observed and predicted values, providing the best-fit line for the data.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "  - R² represents the proportion of variance in the dependent variable explained by the independent variable. It ranges from 0 to 1, where higher values indicate better model fit.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "  - Multiple Linear Regression extends simple linear regression by using two or more independent variables to predict the dependent variable.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        " - Assumptions include:\n",
        "\n",
        "    - Linearity\n",
        "    - Independence\n",
        "    - Homoscedasticity\n",
        "    - Normality of residuals\n",
        "    - No multicollinearity\n",
        "    - No significant outliers or influential points\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "  - Heteroscedasticity refers to non-constant variance of residuals. It leads to inefficient estimates and biased standard errors, making hypothesis tests unreliable.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "  - Use techniques like:\n",
        "    - Removing highly correlated variables\n",
        "    - Principal Component Analysis (PCA)\n",
        "    - Ridge or Lasso regression\n",
        "    - Combining correlated variables\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "  - Common techniques include:\n",
        "    - One-hot encoding\n",
        "    - Label encoding\n",
        "    - Target encoding\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "  - Interaction terms capture the effect of one independent variable on the d dependent variable depending on the level of another variable.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "  - In simple regression, the intercept is the value of Y when X = 0. In multiple regression, the intercept is the expected value of Y when all independent variables are 0 — which may not be meaningful if 0 is outside the data range.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "  - The slope indicates the rate of change in Y for a one-unit change in X. It directly influences the direction and magnitude of predictions.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "  - The intercept gives the baseline value of Y when all predictors are zero, helping interpret the model's starting point.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "Limitations include:\n",
        "  - R² always increases with more variables (even irrelevant ones)\n",
        "  - Doesn't indicate model bias or overfitting\n",
        "  - Doesn't assess predictive accuracy on new data\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "  - A large standard error suggests high variability in the coefficient estimate, indicating uncertainty about its true value. This may lead to non-significant results.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "  - Heteroscedasticity appears as a funnel shape in residual plots (residuals spread out as X increases). It must be addressed because it violates OLS assumptions and affects inference.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - It indicates that many predictors were added, but only a few are useful. Adjusted R² penalizes unnecessary variables, suggesting overfitting.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "  - Scaling ensures that:\n",
        "\n",
        "    - Coefficients are comparable\n",
        "    - Algorithms (like gradient descent) converge faster\n",
        "    - No variable dominates due to scale differences\n",
        "23. What is polynomial regression?\n",
        "  - Polynomial regression models the relationship between X and Y as an nth-degree polynomial, allowing for curved relationships.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "  - Linear regression assumes a straight-line relationship, while polynomial regression allows for curved relationships by including powers of X.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "  - When the relationship between X and Y is non-linear, such as in exponential growth, decay, or U-shaped patterns.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "  - Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\dots + \\beta_nX^n + \\epsilon\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, multivariate polynomial regression can be used, where interactions and higher-order terms are included across multiple variables.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "  - Limitations include:\n",
        "\n",
        "    - Overfitting with high-degree polynomials\n",
        "    - Poor extrapolation\n",
        "    - Interpretability issues\n",
        "    - Sensitive to outliers\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "  - Use:\n",
        "\n",
        "    - Cross-validation\n",
        "    - AIC/BIC\n",
        "    - Adjusted R²\n",
        "    - Residual plots\n",
        "30. Why is visualization important in polynomial regression?\n",
        "  - Visualization helps:\n",
        "    - Assess model fit\n",
        "    - Detect overfitting\n",
        "    - Understand curvature\n",
        "    - Validate assumptions"
      ],
      "metadata": {
        "id": "sBM5R5RGm0g-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6edepQGkcwdF"
      },
      "outputs": [],
      "source": [
        "#31 . How is polynomial regression implemented in Python?\n",
        "'''Use PolynomialFeatures from sklearn.preprocessing to generate polynomial terms, then fit a linear regression model:'''\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Example\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)"
      ]
    }
  ]
}