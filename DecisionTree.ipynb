{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ASSIGNMENT"
      ],
      "metadata": {
        "id": "xHzQbYSP1Epa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work in the context of classification?\n",
        "  - A Decision Tree is a supervised machine learning algorithm that creates a tree-like model of decisions based on feature values. It works by recursively splitting the dataset into subsets based on the most significant feature at each node, creating a hierarchical structure of decision rules.\n",
        "\n",
        "  - In classification, the tree starts at the root node with the entire dataset and selects the best feature to split on (using metrics like Gini Impurity or Entropy). This process continues recursively until reaching leaf nodes that represent class labels. To classify a new instance, it traverses the tree from root to leaf following the decision rules, ultimately assigning it to a class.\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "  - Gini Impurity measures the probability of incorrectly classifying a randomly chosen element. Formula: Gini = 1 - Σ(pi²), where pi is the probability of class i. It ranges from 0 (pure node) to 0.5 (maximum impurity for binary classification).\n",
        "\n",
        "  - Entropy measures the disorder or uncertainty in the data. Formula: Entropy = -Σ(pi × log₂(pi)). It ranges from 0 (pure node) to 1 (maximum uncertainty for binary classification).\n",
        "\n",
        "  - Impact on splits: Both measures help determine the best feature to split on. The algorithm calculates the impurity reduction (Information Gain) for each possible split and chooses the one that maximizes this reduction. Gini is computationally faster, while Entropy tends to produce more balanced trees.\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "  - Pre-Pruning (Early Stopping): Stops tree growth during construction by setting constraints like max_depth, min_samples_split, or min_samples_leaf. The tree building stops when these conditions are met.\n",
        "\n",
        "  - Post-Pruning: Allows the tree to grow fully, then removes branches that provide little predictive power by working backward from leaf nodes.\n",
        "\n",
        "  - Advantages:\n",
        "\n",
        "    - Pre-Pruning: Computationally efficient as it prevents unnecessary tree growth, saving time and memory during training.\n",
        "    - Post-Pruning: Generally produces better accuracy as it first examines the full structure before making pruning decisions, avoiding premature stopping.    \n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "  - Information Gain measures the reduction in entropy (or impurity) achieved by splitting the dataset on a particular feature. Formula: IG = Entropy(parent) - Weighted Average of Entropy(children).\n",
        "\n",
        "  - It's important because it quantifies how much a feature reduces uncertainty about the class labels. The algorithm selects the feature with the highest Information Gain at each node, ensuring that each split maximally separates the classes. This greedy approach leads to more efficient trees that require fewer splits to achieve good classification accuracy.  \n",
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "  - Applications:\n",
        "\n",
        "    - Medical diagnosis (disease prediction)\n",
        "    - Credit risk assessment in banking\n",
        "    - Customer churn prediction\n",
        "Fraud detection\n",
        "    - Marketing campaign targeting\n",
        "  - Advantages:\n",
        "\n",
        "    - Easy to understand and interpret (white-box model)\n",
        "    - Requires minimal data preprocessing\n",
        "    - Handles both numerical and categorical data\n",
        "    - Non-parametric (no assumptions about data distribution)\n",
        "    - Feature importance ranking\n",
        "  - Limitations:  \n",
        "    - Prone to overfitting, especially with deep trees\n",
        "    - Unstable - small data changes can create very different trees\n",
        "    - Biased toward features with many levels\n",
        "    - Can create overly complex trees that don't generalize well\n",
        "    - Not ideal for capturing linear relationships .\n",
        "     "
      ],
      "metadata": {
        "id": "PNYlQqZn1I0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w0FcdQc0xrI"
      },
      "outputs": [],
      "source": [
        "#6. Python program - Iris Dataset with Gini criterion\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Decision Tree Classifier (Gini Criterion)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "for i, importance in enumerate(clf.feature_importances_):\n",
        "    print(f\"{iris.feature_names[i]}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Python program - Compare max_depth=3 vs fully-grown tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "\n",
        "print(\"Decision Tree Comparison\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Fully-grown Tree:\")\n",
        "print(f\"  - Depth: {clf_full.get_depth()}\")\n",
        "print(f\"  - Number of leaves: {clf_full.get_n_leaves()}\")\n",
        "print(f\"  - Accuracy: {accuracy_full:.4f} ({accuracy_full*100:.2f}%)\")\n",
        "print()\n",
        "print(f\"Pruned Tree (max_depth=3):\")\n",
        "print(f\"  - Depth: {clf_pruned.get_depth()}\")\n",
        "print(f\"  - Number of leaves: {clf_pruned.get_n_leaves()}\")\n",
        "print(f\"  - Accuracy: {accuracy_pruned:.4f} ({accuracy_pruned*100:.2f}%)\")\n",
        "print()\n",
        "print(f\"Accuracy difference: {abs(accuracy_full - accuracy_pruned):.4f}\")"
      ],
      "metadata": {
        "id": "zeFnIYRB2pPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Python program - Boston Housing Dataset with Decision Tree Regressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "\n",
        "print(\"Decision Tree Regressor - California Housing Dataset\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "for i, importance in enumerate(regressor.feature_importances_):\n",
        "    print(f\"{housing.feature_names[i]}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "Wclsyttr23fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Python program - GridSearchCV for hyperparameter tuning\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, None],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=clf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Performing Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "\n",
        "best_clf = grid_search.best_estimator_\n",
        "y_pred = best_clf.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"\\nGrid Search Results\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Best Parameters:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"  - {param}: {value}\")\n",
        "print(f\"\\nBest Cross-Validation Accuracy: {best_score:.4f} ({best_score*100:.2f}%)\")\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "A1rwUbuv3Gzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "  - Step-by-Step Process:\n",
        "  - 1. Handle Missing Values:\n",
        "\n",
        "    - Analysis: Identify missing data patterns using df.isnull().sum() and visualizations\n",
        "    - Strategies:\n",
        "    - Numerical features: Impute with mean/median (median for skewed data) or use KNN imputation\n",
        "    - Categorical features: Impute with mode or create \"Unknown\" category\n",
        "    - Consider MICE (Multiple Imputation by Chained Equations) for complex patterns\n",
        "    - If >40% missing in a feature, consider dropping it\n",
        "    - Document all imputation decisions\n",
        "  - 2. Encode Categorical Features:\n",
        "\n",
        "    - Label Encoding: For ordinal variables (e.g., severity: low, medium, high)\n",
        "    - One-Hot Encoding: For nominal variables with few categories (e.g., gender, blood type)\n",
        "    - Target Encoding: For high-cardinality features (e.g., zip codes)\n",
        "    - Handle rare categories by grouping into \"Other\"\n",
        "    - Decision Trees handle encoded features well without scaling"
      ],
      "metadata": {
        "id": "RJk1HVzy3dvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train Decision Tree Model :\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LTCjBlcI4GA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 4. Tune Hyperparameters :     \n",
        "    - Use GridSearchCV or RandomizedSearchCV\n",
        "    - Key parameters to tune:\n",
        "     - max_depth: Control tree depth (3-10)\n",
        "    - min_samples_split: Minimum samples to split (2-20)\n",
        "    - min_samples_leaf: Minimum samples in leaf (1-10)\n",
        "    - max_features: Features to consider for splits\n",
        "    - class_weight: Handle imbalanced classes\n",
        "    - Use cross-validation (5-10 folds) for robust evaluation\n",
        "  - 5. Evaluate Performance:\n",
        "    - Metrics for healthcare (where false negatives are costly):\n",
        "    - Accuracy (overall correctness)\n",
        "    - Recall/Sensitivity: Critical for disease detection (minimize false negatives)\n",
        "    - Precision: Minimize false positives\n",
        "    - F1-Score: Balance between precision and recall\n",
        "    - ROC-AUC: Overall discriminative ability\n",
        "    - Confusion Matrix: Detailed error analysis\n",
        "    - Perform cross-validation for stability assessment\n",
        "    - Test on holdout set for final evaluation\n",
        "    - Feature importance analysis for clinical insights."
      ],
      "metadata": {
        "id": "QQWlfR7D4U2W"
      }
    }
  ]
}