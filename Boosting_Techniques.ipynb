{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques"
      ],
      "metadata": {
        "id": "v__iPgDe2MCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "  - Boosting is an ensemble learning method that combines several \"weak learners\" to create a single \"strong learner\". A weak learner is a model that is only slightly better than random guessing (like a shallow decision tree).\n",
        "\n",
        "\n",
        "  - Sequential Learning: Unlike bagging, boosting trains models sequentially.\n",
        "\n",
        "  - Error Correction: Each new model attempts to correct the errors made by the previous models in the sequence.\n",
        "\n",
        "  - Weight Adjustment: It improves weak learners by assigning higher weights to observations that were misclassified or had high residuals in earlier rounds, forcing the next model to focus more on those \"difficult\" cases.\n",
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "  - Focus : Focuses on misclassified points by increasing their weights.  Focuses on residuals (the difference between predicted and actual values).\n",
        "  - Optimization : Minimizes loss by changing weights of data points. Minimizes loss using Gradient Descent on a loss function.\n",
        "  - Model Building : Usually uses \"Stumps\" (trees with only one split). Uses larger trees, though still relatively shallow.  \n",
        "3. How does regularization help in XGBoost?\n",
        "  - XGBoost (Extreme Gradient Boosting) includes built-in L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "  - Prevents Overfitting: By penalizing complex models, regularization ensures the model doesn't \"memorize\" noise in the training data.\n",
        "\n",
        "  - Complexity Control: It limits the influence of individual features and the depth/number of leaves in the trees, leading to better generalization on unseen data.  \n",
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "  - CatBoost is designed specifically to handle categorical features without requiring manual preprocessing like One-Hot Encoding.\n",
        "\n",
        "  - Ordered Boosting: It uses a proprietary algorithm to handle categorical features by calculating a \"target statistic\" while avoiding data leakage.\n",
        "\n",
        "  - Automatic Encoding: It transforms categorical values into numerical features during training, which reduces memory usage and speeds up the process compared to traditional encoding methods.\n",
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "  - Boosting is often preferred when the goal is high precision and the dataset has a clear but complex structure.\n",
        "\n",
        "  - Fraud Detection: Identifying rare fraudulent transactions among millions of legitimate ones.\n",
        "\n",
        "  - Search Engine Ranking: Ranking pages based on relevance where small errors significantly impact user experience.\n",
        "\n",
        "  - Click-Through Rate (CTR) Prediction: In digital advertising, where predicting the subtle probability of a user clicking is crucial.    \n",
        "  \n"
      ],
      "metadata": {
        "id": "QteN2o8t2OPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I9BUbhN1Kmo"
      },
      "outputs": [],
      "source": [
        "'''#6. Write a Python program to:\n",
        "  - 1. Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "  - 2. Print the model accuracy'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''7.Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(f\"Gradient Boosting R-squared: {r2_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "j84-yWYd4P4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''8.Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy'''\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "6oKL0xBH4a4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''9.Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn'''\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "cat_model = CatBoostClassifier(iterations=100, verbose=0)\n",
        "cat_model.fit(X_train, y_train)\n",
        "y_pred = cat_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K8GDTtFS4sBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "  - 1. Data preprocessing & handling missing/categorical values\n",
        "  - 2. Choice between AdaBoost, XGBoost, or CatBoost\n",
        "  - 3. Hyperparameter tuning strategy\n",
        "  - 4. Evaluation metrics you'd choose and why\n",
        "  - 5. How the business would benefit from your model\n",
        "  - Answer:\n",
        "  - 1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "  - Missing Values: Use Mean/Median imputation for numerical features and Mode imputation or a \"Missing\" label for categorical features.\n",
        "\n",
        "\n",
        "  - Categorical Encoding: Utilize Target Encoding or let the model handle it natively (if using CatBoost) to preserve the relationship between demographics and default risk.\n",
        "\n",
        "\n",
        "  - Imbalance Handling: Apply SMOTE (Synthetic Minority Over-sampling Technique) or adjust the scale_pos_weight parameter within the boosting algorithm to ensure the model doesn't ignore the minority \"default\" class.\n",
        "\n",
        "  - 2. Choice of Model: CatBoost\n",
        "\n",
        "  - Reasoning: While XGBoost and AdaBoost are powerful, CatBoost is preferred here because it handles categorical variables natively without needing manual One-Hot Encoding.\n",
        "\n",
        "\n",
        "  - Efficiency: It is robust against overfitting and deals effectively with the missing values often found in transaction behavior datasets.\n",
        "\n",
        "  - 3. Hyperparameter Tuning Strategy\n",
        "\n",
        "  - Method: Use GridSearchCV or RandomizedSearchCV to find the optimal balance between model complexity and performance.\n",
        "\n",
        "\n",
        "  - Parameters to Tune: Focus on learning_rate (to control step size), depth (to control tree complexity), and l2_leaf_reg (to apply regularization and prevent overfitting).\n",
        "\n",
        "  - 4. Evaluation Metrics\n",
        "\n",
        "  - Primary Metric: F1-Score or Precision-Recall AUC.\n",
        "\n",
        "  - Justification: In loan defaults, Accuracy is misleading because most people don't default. We care about the trade-off between Precision (not rejecting good customers) and Recall (catching as many potential defaulters as possible).\n",
        "\n",
        "  - 5. Business Benefits\n",
        "\n",
        "  - Risk Mitigation: By identifying high-risk individuals early, the company reduces the \"Non-Performing Assets\" (NPA) ratio.\n",
        "\n",
        "\n",
        "  - Automated Decisions: The model enables faster loan approvals for low-risk customers, improving user experience and operational efficiency.\n"
      ],
      "metadata": {
        "id": "JL681Ltp44QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "cat_features_indices = [0, 1, 4]\n",
        "model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    cat_features=cat_features_indices,\n",
        "    verbose=100\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred)) [cite: 66, 67]"
      ],
      "metadata": {
        "id": "xuD7mXq-7ZoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}